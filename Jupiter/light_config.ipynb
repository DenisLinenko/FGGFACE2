{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from os import listdir\n",
    "from os.path import isdir\n",
    "from numpy import savez_compressed\n",
    "from matplotlib import pyplot\n",
    "from PIL import Image\n",
    "from numpy import asarray\n",
    "from scipy.spatial.distance import cosine\n",
    "from mtcnn.mtcnn import MTCNN\n",
    "from numpy import load\n",
    "from keras_vggface.vggface import VGGFace\n",
    "from keras_vggface.utils import preprocess_input\n",
    "from time import perf_counter\n",
    "import sys\n",
    "import numpy as np\n",
    "import io\n",
    "import json\n",
    "import math\n",
    "import matplotlib.image as mpimg\n",
    "from matplotlib import rcParams\n",
    "import matplotlib.pyplot as plt\n",
    "import ast\n",
    "import tensorflow as tf\n",
    "from numpy import expand_dims\n",
    "from keras.models import load_model\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.svm import SVC\n",
    "from scipy.spatial import distance\n",
    "import scipy\n",
    "import imagehash\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file():\n",
    "    file = open(\"test_data.txt\", \"r\")\n",
    "    contents = file.read()\n",
    "    dictionary = ast.literal_eval(contents)\n",
    "    file.close()\n",
    "    return dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_images_matching_criteria(criteria):\n",
    "    dictionary1 = read_file()\n",
    "    known_image = ''\n",
    "    test_image = ''\n",
    "    for key, value in dictionary1.items():\n",
    "        if(key == criteria):\n",
    "            known_image = value[0]\n",
    "            test_image = value[1]\n",
    "    known_image_path = 'known/'+known_image+'.jpg' \n",
    "    test_image_path = 'test/'+test_image+'.jpg'\n",
    "    return known_image_path, test_image_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# extract a single face from a given photograph\n",
    "def extract_face(filename, required_size=(224, 224)):\n",
    "\t# load image from file\n",
    "\tpixels = pyplot.imread(filename)\n",
    "\t# create the detector, using default weights\n",
    "\tdetector = MTCNN()\n",
    "\t# detect faces in the image\n",
    "\tresults = detector.detect_faces(pixels)\n",
    "\n",
    "\tprint(len(results), 'faces have been found')\n",
    "#--------------------------- detecting the number of faces ------------------------------------\n",
    "\tif len(results) == 0:\n",
    "\t\traise Exception('No faces detected')\n",
    "\telif len(results) > 1:\n",
    "\t\traise Exception('Multiple faces detected')\t\t\n",
    "        \n",
    "\t# extract the bounding box from the first face\n",
    "\tx1, y1, width, height = results[0]['box']\n",
    "\tx2, y2 = x1 + width, y1 + height\n",
    "\t# extract the face\n",
    "\tface = pixels[y1:y2, x1:x2]\n",
    "\t# resize pixels to the model size\n",
    "\timage = Image.fromarray(face)\n",
    "\timage = image.resize(required_size)\n",
    "\tface_array = asarray(image)\n",
    "\n",
    "\treturn face_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embeddings_l(filenames):\n",
    "    # extract faces\n",
    "    faces = [extract_face(f) for f in filenames]\n",
    "    # convert into an array of samples\n",
    "    samples = asarray(faces, 'float32')\n",
    "    # prepare the face for the model, e.g. center pixels\n",
    "    samples = preprocess_input(samples, version=2)\n",
    "    \n",
    "    # Load the TFLite model and allocate tensors. View details\n",
    "    \n",
    "    interpreter = tf.lite.Interpreter(model_path=\"model_resnet50.tflite\")\n",
    "    interpreter.allocate_tensors()\n",
    "\n",
    "    # Get input and output tensors.\n",
    "    input_details = interpreter.get_input_details()\n",
    "    output_details = interpreter.get_output_details()\n",
    "\n",
    "    # Test the model on input data.\n",
    "    # input_shape = input_details[0]['shape']\n",
    "    \n",
    "    # Use same image as Keras model\n",
    "    input_data = np.array(samples, dtype=np.float32)\n",
    "    interpreter.set_tensor(input_details[0]['index'], input_data)\n",
    "    interpreter.get_tensor_details()\n",
    "\n",
    "    interpreter.invoke()\n",
    "\n",
    "    # The function `get_tensor()` returns a copy of the tensor data.\n",
    "    # Use `tensor()` in order to get a pointer to the tensor.\n",
    "    output_data = interpreter.get_tensor(output_details[0]['index'])\n",
    "    \n",
    "    return output_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embeddings_l_vgg16(filenames):\n",
    "    # extract faces\n",
    "    faces = [extract_face(f) for f in filenames]\n",
    "    # convert into an array of samples\n",
    "    samples = asarray(faces, 'float32')\n",
    "    # prepare the face for the model, e.g. center pixels\n",
    "    samples = preprocess_input(samples, version=2)\n",
    "    \n",
    "    # Load the TFLite model and allocate tensors. View details\n",
    "    \n",
    "    interpreter = tf.lite.Interpreter(model_path=\"model_vgg16.tflite\")\n",
    "    interpreter.allocate_tensors()\n",
    "\n",
    "    # Get input and output tensors.\n",
    "    input_details = interpreter.get_input_details()\n",
    "    output_details = interpreter.get_output_details()\n",
    "\n",
    "    # Test the model on input data.\n",
    "    # input_shape = input_details[0]['shape']\n",
    "    \n",
    "    # Use same image as Keras model\n",
    "    input_data = np.array(samples, dtype=np.float32)\n",
    "    interpreter.set_tensor(input_details[0]['index'], input_data)\n",
    "    interpreter.get_tensor_details()\n",
    "\n",
    "    interpreter.invoke()\n",
    "\n",
    "    # The function `get_tensor()` returns a copy of the tensor data.\n",
    "    # Use `tensor()` in order to get a pointer to the tensor.\n",
    "    output_data = interpreter.get_tensor(output_details[0]['index'])\n",
    "    \n",
    "    return output_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embeddings(filenames):\n",
    "    # extract faces\n",
    "    faces = [extract_face(f) for f in filenames]\n",
    "    # convert into an array of samples\n",
    "    samples = asarray(faces, 'float32')\n",
    "    # prepare the face for the model, e.g. center pixels\n",
    "    samples = preprocess_input(samples, version=2)\n",
    "    # create predication model - note that we have several \n",
    "    model = VGGFace(model='resnet50', include_top=False, input_shape=(224, 224, 3), pooling='avg')\n",
    "    # perform prediction\n",
    "    yhat = model.predict(samples)\n",
    "    return yhat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_initial_images(path_known, path_test):\n",
    "    \n",
    "    rcParams['figure.figsize'] = 13 ,10\n",
    "    img_A = mpimg.imread(path_known)\n",
    "    img_B = mpimg.imread(path_test)\n",
    "    fig, ax = plt.subplots(1,2)\n",
    "    ax[0].imshow(img_A);\n",
    "    ax[1].imshow(img_B);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_match(known_embedding, candidate_embedding, thresh=0.5):\n",
    "    # calculate distance between embeddings\n",
    "    print(\"-------------------------------------------\")\n",
    "    print(\"Comparing two embeddings, using \\\"vggface-model\\\" we got the result:\")\n",
    "    score = cosine(known_embedding, candidate_embedding)\n",
    "    if score <= thresh:\t\n",
    "        \n",
    "        print('>face is a Match (%.3f <= %.3f) = %.0f' % (score, thresh, math.floor((1-score)*100)), \"%\")\n",
    "        return True\n",
    "    else:\n",
    "        print('>face is NOT a Match (%.3f <= %.3f) = %.0f' % (score, thresh, math.floor((1-score)*100)), \"%\")\n",
    "        return False\n",
    "    #print(\"-------------------------------------------\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_match_mass(known_embedding, candidate_embedding, thresh=0.5):\n",
    "    score = cosine(known_embedding, candidate_embedding)\n",
    "    return math.floor((1-score)*100)\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_match_light(known_embedding, candidate_embedding, thresh=0.5):\n",
    "    # calculate distance between embeddings\n",
    "    print(\"-------------------------------------------\")\n",
    "    print(\"Comparing two embeddings, using \\\"hybrid resnet50-model\\\" we got the result:\")\n",
    "    score = cosine(known_embedding, candidate_embedding)\n",
    "    if score <= thresh:\t\n",
    "        print('>face is a Match (%.3f <= %.3f) = %.0f' % (score, thresh, math.floor((1-score)*100)), \"%\")\n",
    "    else:\n",
    "        print('>face is NOT a Match (%.3f <= %.3f) = %.0f' % (score, thresh, math.floor((1-score)*100)), \"%\")\n",
    "    print(\"-------------------------------------------\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_match_light_vgg16(known_embedding, candidate_embedding, thresh=0.5):\n",
    "    # calculate distance between embeddings\n",
    "    print(\"-------------------------------------------\")\n",
    "    print(\"Comparing two embeddings, using \\\"hybrid vgg16-model\\\" we got the result:\")\n",
    "    score = cosine(known_embedding, candidate_embedding)\n",
    "    if score <= thresh:\t\n",
    "        print('>face is a Match (%.3f <= %.3f) = %.0f' % (score, thresh, math.floor((1-score)*100)), \"%\")\n",
    "    else:\n",
    "        print('>face is NOT a Match (%.3f <= %.3f) = %.0f' % (score, thresh, math.floor((1-score)*100)), \"%\")\n",
    "    print(\"-------------------------------------------\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare(image1, image2):\n",
    "    vector1 = get_embeddings([image1])[0]\n",
    "    vector2 = get_embeddings([image2])[0]\n",
    "    vector1_light = get_embeddings_l([image1])[0]\n",
    "    vector2_light = get_embeddings_l([image2])[0]\n",
    "    vector1_light_vgg16 = get_embeddings_l_vgg16([image1])[0]\n",
    "    vector2_light_vgg16 = get_embeddings_l_vgg16([image2])[0]\n",
    "    is_match(vector1, vector2)        \n",
    "    is_match_light(vector1_light, vector2_light)\n",
    "    is_match_light_vgg16(vector1_light_vgg16, vector2_light_vgg16)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hash_comparing(known_image, test_image):\n",
    "    hash = imagehash.average_hash(Image.open(known_image))\n",
    "    otherhash = imagehash.average_hash(Image.open(test_image))\n",
    "    print('result of comparing: ', hash == otherhash)\n",
    "    print(hash - otherhash)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.3 64-bit",
   "language": "python",
   "name": "python37364bit1dae926334f64521bdc7fa7747baedd6"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
